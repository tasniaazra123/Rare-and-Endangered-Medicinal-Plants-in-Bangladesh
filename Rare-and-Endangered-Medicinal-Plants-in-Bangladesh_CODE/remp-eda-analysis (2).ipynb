{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13417262,"sourceType":"datasetVersion","datasetId":8515733}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**<h2>Import libaries</h2>**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Image processing\nfrom PIL import Image\nimport cv2\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Display settings for better output\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\nnp.set_printoptions(suppress=True)\n\nprint(\"All libraries imported successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:30.263767Z","iopub.execute_input":"2025-10-25T20:10:30.264106Z","iopub.status.idle":"2025-10-25T20:10:30.273273Z","shell.execute_reply.started":"2025-10-25T20:10:30.264083Z","shell.execute_reply":"2025-10-25T20:10:30.272256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2>Dataset setup</h2>**","metadata":{}},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/rare-and-endangered-medicinal-plants-in-bangladesh/Raw-Images\"\n\n# Verify the dataset exists\nif os.path.exists(dataset_path):\n    print(f\"Dataset found at: {dataset_path}\")\nelse:\n    print(f\"Dataset not found at: {dataset_path}\")\n    # Try alternative path (common variations)\n    dataset_path = \"/kaggle/input/remp-dataset/Raw-Images\"\n    if os.path.exists(dataset_path):\n        print(f\"Dataset found at alternative path: {dataset_path}\")\n    else:\n        print(\"Please check your dataset path\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:30.274695Z","iopub.execute_input":"2025-10-25T20:10:30.274965Z","iopub.status.idle":"2025-10-25T20:10:30.281574Z","shell.execute_reply.started":"2025-10-25T20:10:30.274937Z","shell.execute_reply":"2025-10-25T20:10:30.280699Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2>Image Reading</h2>**","metadata":{}},{"cell_type":"code","source":"print(\"\\nTesting image reading:\\n\")\nsample_success = 0\nsample_failed = 0\n\nfor class_name in classes[:3]:  # Test first 3 classes\n    class_path = os.path.join(dataset_path, class_name)\n    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    \n    if images:\n        sample_path = os.path.join(class_path, images[0])\n        try:\n            with Image.open(sample_path) as img:\n                width, height = img.size\n                print(f\"  {class_name}: Sample image {width}x{height} - Read successfully\")\n                sample_success += 1\n        except Exception as e:\n            print(f\"  {class_name}: Failed to read - {e}\")\n            sample_failed += 1\n\nprint(f\"\\nImage reading test: {sample_success} successful, {sample_failed} failed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:30.282370Z","iopub.execute_input":"2025-10-25T20:10:30.282632Z","iopub.status.idle":"2025-10-25T20:10:30.301450Z","shell.execute_reply.started":"2025-10-25T20:10:30.282613Z","shell.execute_reply":"2025-10-25T20:10:30.300270Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2>Class Balance Analysis</h2>**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# Mapping based on file names\nplant_mapping = {\n    'type_1': 'Kalochitra',\n    'type_2': 'Anantamu', \n    'type_3': 'Apang',\n    'type_4': 'Syspan',\n    'type_5': 'Appishikha',\n    'type_6': 'Kalamegh',\n    'type_7': 'Kalothutura',\n    'type_8': 'Gshnru',\n    'type_9': 'Chapalish',\n    'type_10': 'Beta',\n    'type_11': 'Punarnava',\n    'type_12': 'Bamonhati',\n    'type_13': 'Basok',\n    'type_14': 'Ramtulsi',\n    'type_15': 'Shotomuli',\n    'type_16': 'Sarpagandha'\n}\n\n# Create DataFrame for analysis\nplant_data = []\nfor type_name, plant_name in plant_mapping.items():\n    count = class_counts[type_name]  # From previous code\n    plant_data.append({'Type': type_name, 'Plant_Name': plant_name, 'Count': count})\n\ndf_plants = pd.DataFrame(plant_data)\n\n# Sort by count for better visualization\ndf_plants = df_plants.sort_values('Count', ascending=False)\n\nprint(\"\\033[1mPLANT CLASS DISTRIBUTION ANALYSIS:\\033[0m\")\nprint(\"\")\n\nprint(f\"Total Plants: {len(df_plants)}\")\nprint(f\"Total Images: {df_plants['Count'].sum()}\")\nprint(f\"Average images per plant: {df_plants['Count'].mean():.1f}\")\nprint(f\"Most represented: {df_plants.iloc[0]['Plant_Name']} ({df_plants.iloc[0]['Count']} images)\")\nprint(f\"Least represented: {df_plants.iloc[-1]['Plant_Name']} ({df_plants.iloc[-1]['Count']} images)\")\n\nprint(\"\\033[1m\\nIMAGE DIMENSION STATISTICS:\\033[0m\")\nprint(\"\")\nwidths = []\nheights = []\naspect_ratios = []\nfile_sizes = []\n\n# Collect actual image statistics\nfor class_name in classes:\n    class_path = os.path.join(dataset_path, class_name)\n    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    \n    if images:\n        sample_path = os.path.join(class_path, images[0])\n        try:\n            with Image.open(sample_path) as img:\n                width, height = img.size\n                widths.append(width)\n                heights.append(height)\n                aspect_ratios.append(width / height)\n                file_sizes.append(os.path.getsize(sample_path) / 1024)  # KB\n        except Exception as e:\n            print(f\"Error reading image: {e}\")\n\nif widths and heights:\n    print(f\"Average dimensions: {np.mean(widths):.0f} x {np.mean(heights):.0f} pixels\")\n    print(f\"Min dimensions: {np.min(widths)} x {np.min(heights)} pixels\")\n    print(f\"Max dimensions: {np.max(widths)} x {np.max(heights)} pixels\")\n    print(f\"Median aspect ratio: {np.median(aspect_ratios):.2f}\")\n    print(f\"Average file size: {np.mean(file_sizes):.1f} KB\")\nelse:\n    print(\"No image dimension data available\")\n\n# Calculate imbalance metrics\nmax_count = df_plants['Count'].max()\nmin_count = df_plants['Count'].min()\nimbalance_ratio = max_count / min_count\n\nprint(\"\\033[1m\\nIMBALANCE ANALYSIS:\\033[0m\")\nprint(\"\")\n\nprint(f\"Imbalance Ratio: {imbalance_ratio:.2f}:1\")\nprint(f\"Standard Deviation: {df_plants['Count'].std():.1f}\")\n\nif imbalance_ratio > 5:\n    print(\"SIGNIFICANT CLASS IMBALANCE DETECTED\")\n    print(\"This may bias the model toward majority classes\")\n    print(\"Recommended: Use class weights or oversampling\")\nelif imbalance_ratio > 3:\n    print(\"MODERATE CLASS IMBALANCE PRESENT\")\nelse:\n    print(\"Classes are relatively balanced\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:30.303704Z","iopub.execute_input":"2025-10-25T20:10:30.304044Z","iopub.status.idle":"2025-10-25T20:10:30.348478Z","shell.execute_reply.started":"2025-10-25T20:10:30.304004Z","shell.execute_reply":"2025-10-25T20:10:30.347598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2>Class Balance Visuallization</h2>**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\n\n# Plot 1: Bar chart with plant names\nplt.subplot(2, 1, 1)\nbars = plt.bar(range(len(df_plants)), df_plants['Count'], \n               color=['#2E8B57' if x > 200 else '#FFA07A' for x in df_plants['Count']])\nplt.title('Distribution of Medicinal Plant Images', fontsize=16, fontweight='bold')\nplt.ylabel('Number of Images', fontsize=12)\nplt.xticks(range(len(df_plants)), [f\"{name}\\n({type})\" for name, type in zip(df_plants['Plant_Name'], df_plants['Type'])], \n           rotation=45, ha='right', fontsize=10)\nplt.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor bar, count in zip(bars, df_plants['Count']):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n             str(count), ha='center', va='bottom', fontsize=9)\n\n# Plot 2: Pie chart showing imbalance\nplt.subplot(2, 1, 2)\nplt.pie(df_plants['Count'], labels=df_plants['Plant_Name'], autopct='%1.1f%%', \n        startangle=90, textprops={'fontsize': 8})\nplt.title('Percentage Distribution Across Plant Species', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Detailed statistics\nprint(\"\\033[1m\\nDETAILED CLASS STATISTICS:\\n\\033[0m\")\nprint(df_plants.describe())\n\n# Identify potential biases\nprint(\"\\033[1m\\nPOTENTIAL BIASES IDENTIFIED:\\n\\033[0m\")\nprint(f\"1. Punarnava has {max_count} images (3.7x more than Shotomuli)\")\nprint(f\"2. 6 plants have <150 images, while 4 plants have >250 images\")\nprint(f\"3. The model may perform better on well-represented plants like Punarnava, Kalamegh, Syspan\")\nprint(f\"4. Rare plants like Shotomuli, Bamonhati may be harder to classify accurately\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:30.349368Z","iopub.execute_input":"2025-10-25T20:10:30.349624Z","iopub.status.idle":"2025-10-25T20:10:30.976079Z","shell.execute_reply.started":"2025-10-25T20:10:30.349605Z","shell.execute_reply":"2025-10-25T20:10:30.975073Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2> Sample Images Visualization & Intra-class Variation</h2>**","metadata":{}},{"cell_type":"code","source":"def show_samples(class_id, n=4):\n    \"\"\"Display sample images from a specific plant class\"\"\"\n    path = os.path.join(dataset_path, class_id) \n    all_images = os.listdir(path)\n    \n    if len(all_images) < n:\n        images = all_images\n    else:\n        images = random.sample(all_images, n)\n    \n    plant_name = plant_mapping[class_id]  \n    \n    plt.figure(figsize=(12, 3))\n    for i, img in enumerate(images):\n        img_path = os.path.join(path, img)\n        img_data = Image.open(img_path)\n        plt.subplot(1, n, i+1)\n        plt.imshow(img_data)\n        plt.title(f\"{plant_name}\\n{class_id}\", fontsize=10)\n        plt.axis('off')\n    \n    plt.suptitle(f\"{plant_name} ({len(all_images)} images)\", fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Print observation about this plant's variation\n    diversity_level = \"good\" if len(all_images) > 150 else \"moderate\" if len(all_images) > 100 else \"limited\"\n    print(f\"{plant_name}: Shows {diversity_level} diversity in leaf morphology\")\n\nimport random\n\nfor cid in classes[:16]:  \n    show_samples(cid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:30.977073Z","iopub.execute_input":"2025-10-25T20:10:30.977383Z","iopub.status.idle":"2025-10-25T20:10:39.761756Z","shell.execute_reply.started":"2025-10-25T20:10:30.977362Z","shell.execute_reply":"2025-10-25T20:10:39.760602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2>Image Properties Investigation</h2>**","metadata":{}},{"cell_type":"code","source":"# Analyze properties by plant type\nplant_properties = {}\nall_widths, all_heights, all_aspects, all_sizes = [], [], [], []\n\nfor type_name, plant_name in plant_mapping.items():\n    if type_name not in class_counts:  # Skip if type doesn't exist\n        continue\n        \n    class_path = os.path.join(dataset_path, type_name)\n    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    \n    widths, heights, aspects, file_sizes = [], [], [], []\n    \n    # Sample up to 25 images per plant for analysis\n    sample_size = min(25, len(images))\n    for img_file in images[:sample_size]:\n        img_path = os.path.join(class_path, img_file)\n        try:\n            with Image.open(img_path) as img:\n                width, height = img.size\n                widths.append(width)\n                heights.append(height)\n                aspect = width / height\n                aspects.append(aspect)\n                \n                # Add to global lists\n                all_widths.append(width)\n                all_heights.append(height)\n                all_aspects.append(aspect)\n                \n            file_size = os.path.getsize(img_path) / 1024  # KB\n            file_sizes.append(file_size)\n            all_sizes.append(file_size)\n            \n        except Exception as e:\n            continue\n    \n    if widths:  # If we have valid images\n        plant_properties[plant_name] = {\n            'avg_width': np.mean(widths),\n            'avg_height': np.mean(heights),\n            'std_width': np.std(widths),\n            'std_height': np.std(heights),\n            'min_width': np.min(widths),\n            'max_width': np.max(widths),\n            'min_height': np.min(heights),\n            'max_height': np.max(heights),\n            'avg_aspect': np.mean(aspects),\n            'std_aspect': np.std(aspects),\n            'avg_size_kb': np.mean(file_sizes),\n            'total_pixels': np.mean([w*h for w,h in zip(widths, heights)]),\n            'resolution_consistency': (np.std(widths) / np.mean(widths) + np.std(heights) / np.mean(heights)) * 100\n        }\n\n# Convert to DataFrame for analysis\ndf_properties = pd.DataFrame(plant_properties).T\n\nprint(\"\\033[1m\\nDETAILED IMAGE PROPERTIES BY PLANT SPECIES:\\n\\033[0m\")\nprint(df_properties[['avg_width', 'avg_height', 'avg_aspect', 'avg_size_kb', 'resolution_consistency']].round(2),\"\\n\")\n\n# Create enhanced visualization\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\n\n# 1. Resolution scatter plot with error bars\nfor i, plant in enumerate(df_properties.index):\n    axes[0,0].errorbar(df_properties.loc[plant, 'avg_width'], \n                      df_properties.loc[plant, 'avg_height'],\n                      xerr=df_properties.loc[plant, 'std_width'],\n                      yerr=df_properties.loc[plant, 'std_height'],\n                      fmt='o', alpha=0.7, label=plant, markersize=8,\n                      capsize=3, capthick=1)\n\naxes[0,0].set_title('A) Average Image Dimensions with Variability', fontweight='bold')\naxes[0,0].set_xlabel('Width (pixels)')\naxes[0,0].set_ylabel('Height (pixels)')\naxes[0,0].grid(alpha=0.3)\naxes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# 2. Aspect ratio distribution with variability\ny_pos = np.arange(len(df_properties))\naxes[0,1].barh(y_pos, df_properties['avg_aspect'], xerr=df_properties['std_aspect'],\n               color='lightblue', alpha=0.7, capsize=3)\naxes[0,1].set_yticks(y_pos)\naxes[0,1].set_yticklabels(df_properties.index)\naxes[0,1].axvline(x=1.0, color='red', linestyle='--', alpha=0.7, label='Square (1:1)')\naxes[0,1].set_title('B) Aspect Ratio Distribution', fontweight='bold')\naxes[0,1].set_xlabel('Aspect Ratio (Width/Height)')\naxes[0,1].legend()\n\n# 3. File size distribution\naxes[0,2].barh(df_properties.index, df_properties['avg_size_kb'], \n               color='lightgreen', alpha=0.7)\naxes[0,2].set_title('C) Average File Size by Species', fontweight='bold')\naxes[0,2].set_xlabel('File Size (KB)')\n\n# 4. Resolution consistency (Coefficient of Variation - lower is better)\nconsistency_sorted = df_properties.sort_values('resolution_consistency')\naxes[1,0].barh(consistency_sorted.index, consistency_sorted['resolution_consistency'],\n               color='lightcoral', alpha=0.7)\naxes[1,0].axvline(x=10, color='red', linestyle='--', label='Good consistency threshold')\naxes[1,0].set_title('D) Resolution Consistency\\n(Coefficient of Variation % - Lower = Better)', fontweight='bold')\naxes[1,0].set_xlabel('CV (%) = (Std/Mean) × 100')\naxes[1,0].legend()\n\n# 5. Total pixels (resolution) comparison\naxes[1,1].barh(df_properties.index, df_properties['total_pixels'] / 1000, \n               color='plum', alpha=0.7)\naxes[1,1].set_title('E) Average Total Pixels (Thousands)', fontweight='bold')\naxes[1,1].set_xlabel('Pixels (×1000)')\n\n# 6. Resolution range visualization\nfor i, plant in enumerate(df_properties.index):\n    min_w, max_w = df_properties.loc[plant, 'min_width'], df_properties.loc[plant, 'max_width']\n    avg_w = df_properties.loc[plant, 'avg_width']\n    axes[1,2].plot([min_w, max_w], [i, i], 'o-', color='blue', alpha=0.7, linewidth=2)\n    axes[1,2].plot(avg_w, i, 'o', color='red', markersize=6, label='Average' if i==0 else \"\")\n\naxes[1,2].set_yticks(range(len(df_properties)))\naxes[1,2].set_yticklabels(df_properties.index)\naxes[1,2].set_title('F) Width Range by Species\\n(Blue=Range, Red=Average)', fontweight='bold')\naxes[1,2].set_xlabel('Width (pixels)')\naxes[1,2].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Enhanced overall statistics\nprint(\"\\033[1m\\nOVERALL RESOLUTION STATISTICS:\\n\\033[0m\")\nprint(f\"Dataset Average: {np.mean(all_widths):.0f} × {np.mean(all_heights):.0f} pixels\")\nprint(f\"Resolution Range: {np.min(all_widths)}×{np.min(all_heights)} to {np.max(all_widths)}×{np.max(all_heights)}\")\nprint(f\"Aspect Ratio: {np.mean(all_aspects):.2f} ± {np.std(all_aspects):.2f}\")\nprint(f\"File Sizes: {np.min(all_sizes):.1f} - {np.max(all_sizes):.1f} KB (avg: {np.mean(all_sizes):.1f} KB)\")\n\n# Identify plants with resolution issues\nhigh_variability = df_properties[df_properties['resolution_consistency'] > 15]\nif len(high_variability) > 0:\n    print(\"\\033[1m\\nPLANTS WITH HIGH RESOLUTION VARIABILITY (>15% CV):\\n\\033[0m\")\n    for plant in high_variability.index:\n        cv = high_variability.loc[plant, 'resolution_consistency']\n        print(f\"   {plant}: {cv:.1f}% variability\")\n\nprint(\"\\033[1m\\nENHANCED PREPROCESSING STRATEGY RECOMMENDATIONS:\\n\\033[0m\")\n\n# Determine optimal resize based on analysis\navg_resolution = (np.mean(all_widths), np.mean(all_heights))\nif avg_resolution[0] > 500 or avg_resolution[1] > 500:\n    recommended_size = \"256×256 or 224×224\"\nelif avg_resolution[0] > 300 or avg_resolution[1] > 300:\n    recommended_size = \"224×224\" \nelse:\n    recommended_size = \"128×128\"\n\nprint(f\"1. RESIZING: Standardize to {recommended_size} for optimal CNN performance\")\nprint(f\"   Current average: {np.mean(all_widths):.0f}×{np.mean(all_heights):.0f}\")\nprint(f\"   Resizing will reduce computational requirements while maintaining features\")\n\nprint(f\"2. NORMALIZATION: Use ImageNet statistics for transfer learning\")\nprint(f\"   Mean: [0.485, 0.456, 0.406], Std: [0.229, 0.224, 0.225]\")\n\nprint(f\"3. DATA AUGMENTATION:\")\nprint(f\"   Account for natural resolution variations\")\nprint(f\"   Use center cropping for consistent input sizes\")\nprint(f\"   Maintain aspect ratio during initial processing\")\n\nprint(f\"4. QUALITY CONTROL:\")\nprint(f\"   Monitor plants with high resolution variability: {', '.join(high_variability.index) if len(high_variability) > 0 else 'None'}\")\nprint(f\"   Ensure consistent preprocessing across all species\")\n\nprint(f\"5. MEMORY OPTIMIZATION:\")\nprint(f\"   Average file size: {np.mean(all_sizes):.1f} KB -> Efficient for batch processing\")\nprint(f\"   Recommended batch size: 32-64 for 224×224 images\")\n\n# Technical insights for model selection\nprint(\"\\033[1m\\nTECHNICAL INSIGHTS FOR MODEL DEVELOPMENT:\\n\\033[0m\")\nprint(f\"Input shape compatibility: All major CNN architectures support {recommended_size} inputs\")\nprint(f\"Memory requirements: ~{len(plant_mapping) * 50}MB for model + features\")\nprint(f\"Training time: Moderate (2-4 hours on GPU for full fine-tuning)\")\nprint(f\"Potential issues: Resolution variability may affect some species more than others\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:39.764225Z","iopub.execute_input":"2025-10-25T20:10:39.764545Z","iopub.status.idle":"2025-10-25T20:10:41.854291Z","shell.execute_reply.started":"2025-10-25T20:10:39.764521Z","shell.execute_reply":"2025-10-25T20:10:41.853202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2>COLOR DISTRIBUTION ANALYSIS</h2>**","metadata":{}},{"cell_type":"code","source":"print(\"\\033[1m\\nCOLOR DISTRIBUTION ANALYSIS:\\n\\033[0m\")\nR, G, B = [], [], []\ncolor_by_plant = {}\n\nfor class_name in classes:\n    class_path = os.path.join(dataset_path, class_name)\n    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    \n    # Use plant name from mapping instead of undefined class_names\n    plant_name = plant_mapping.get(class_name, class_name)\n    \n    plant_R, plant_G, plant_B = [], [], []\n    \n    # Sample up to 8 images per plant\n    sample_images = images[:8]\n    \n    for imgname in sample_images:\n        try:\n            img_path = os.path.join(class_path, imgname)\n            img = Image.open(img_path).convert('RGB')\n            arr = np.array(img)/255.0\n            plant_R.append(arr[:,:,0].mean())\n            plant_G.append(arr[:,:,1].mean())\n            plant_B.append(arr[:,:,2].mean())\n            \n            R.append(arr[:,:,0].mean())\n            G.append(arr[:,:,1].mean())\n            B.append(arr[:,:,2].mean())\n        except Exception as e:\n            continue\n    \n    if plant_R:  # If we have color data for this plant\n        color_by_plant[plant_name] = {\n            'R': np.mean(plant_R),\n            'G': np.mean(plant_G), \n            'B': np.mean(plant_B)\n        }\n\n# Create enhanced color visualization\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Box plot for overall color distribution\nbox_data = [R, G, B]\naxes[0].boxplot(box_data, labels=['Red', 'Green', 'Blue'])\naxes[0].set_title(\"Overall Color Channel Distribution\", fontweight='bold')\naxes[0].set_ylabel(\"Mean Pixel Value (0-1)\")\naxes[0].grid(alpha=0.3)\n\n# Plant-specific color analysis\nplant_names = list(color_by_plant.keys())\nplant_r = [color_by_plant[p]['R'] for p in plant_names]\nplant_g = [color_by_plant[p]['G'] for p in plant_names] \nplant_b = [color_by_plant[p]['B'] for p in plant_names]\n\nx_pos = np.arange(len(plant_names))\nwidth = 0.25\n\naxes[1].bar(x_pos - width, plant_r, width, label='Red', color='red', alpha=0.7)\naxes[1].bar(x_pos, plant_g, width, label='Green', color='green', alpha=0.7)\naxes[1].bar(x_pos + width, plant_b, width, label='Blue', color='blue', alpha=0.7)\n\naxes[1].set_title(\"Average Color Values by Plant Species\", fontweight='bold')\naxes[1].set_xlabel(\"Plant Species\")\naxes[1].set_ylabel(\"Mean Color Value\")\naxes[1].set_xticks(x_pos)\naxes[1].set_xticklabels(plant_names, rotation=45, ha='right')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\033[1mColor Analysis Insights:\\033[0m\")\nprint(f\"Green channel dominates (mean: {np.mean(G):.3f}) - typical for plant leaves\")\nprint(f\"Color variance across plants: R={np.std(R):.3f}, G={np.std(G):.3f}, B={np.std(B):.3f}\")\nprint(f\"Some plants show distinct color profiles useful for identification\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:41.855253Z","iopub.execute_input":"2025-10-25T20:10:41.855548Z","iopub.status.idle":"2025-10-25T20:10:43.541953Z","shell.execute_reply.started":"2025-10-25T20:10:41.855526Z","shell.execute_reply":"2025-10-25T20:10:43.540973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2>ADVANCED COLOR SPACE ANALYSIS (HSV & LAB)</h2>**","metadata":{}},{"cell_type":"code","source":"print(\"\\033[1m\\nADVANCED COLOR SPACE ANALYSIS (HSV & LAB):\\n\\033[0m\")\nprint(\"HSV & LAB color spaces can reveal plant health and species-specific characteristics\")\n\nH_vals, S_vals, V_vals = [], [], []\nL_vals, A_vals, B_vals = [], [], []\ncolor_by_plant_advanced = {}\n\nfor type_name, plant_name in plant_mapping.items():\n    if type_name not in class_counts:  # Skip if type doesn't exist\n        continue\n        \n    class_path = os.path.join(dataset_path, type_name)\n    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    \n    plant_H, plant_S, plant_V = [], [], []\n    plant_L, plant_A, plant_B = [], [], []\n    \n    # Sample 5 images per plant\n    sample_images = images[:5]\n    \n    for img_name in sample_images:\n        try:\n            img_path = os.path.join(class_path, img_name)\n            img = cv2.imread(img_path)\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n            # Convert to HSV and LAB\n            hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n            lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n            \n            # Collect channel means\n            plant_H.append(hsv[:,:,0].mean())\n            plant_S.append(hsv[:,:,1].mean()) \n            plant_V.append(hsv[:,:,2].mean())\n            plant_L.append(lab[:,:,0].mean())\n            plant_A.append(lab[:,:,1].mean())\n            plant_B.append(lab[:,:,2].mean())\n            \n            # Add to overall lists\n            H_vals.append(hsv[:,:,0].mean())\n            S_vals.append(hsv[:,:,1].mean())\n            V_vals.append(hsv[:,:,2].mean())\n            L_vals.append(lab[:,:,0].mean())\n            A_vals.append(lab[:,:,1].mean())\n            B_vals.append(lab[:,:,2].mean())\n            \n        except Exception as e:\n            continue\n    \n    if plant_H:  # If we have data for this plant\n        color_by_plant_advanced[plant_name] = {\n            'H': np.mean(plant_H), 'S': np.mean(plant_S), 'V': np.mean(plant_V),\n            'L': np.mean(plant_L), 'A': np.mean(plant_A), 'B': np.mean(plant_B)\n        }\n\n# Create comprehensive color space visualization\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# HSV Distribution\nhsv_data = [H_vals, S_vals, V_vals]\naxes[0,0].boxplot(hsv_data, labels=['Hue', 'Saturation', 'Value'])\naxes[0,0].set_title(\"HSV Color Space Distribution\", fontweight='bold')\naxes[0,0].set_ylabel(\"Mean Channel Value\")\naxes[0,0].grid(alpha=0.3)\n\n# LAB Distribution  \nlab_data = [L_vals, A_vals, B_vals]\naxes[0,1].boxplot(lab_data, labels=['L* (Lightness)', 'a* (Green-Red)', 'b* (Blue-Yellow)'])\naxes[0,1].set_title(\"LAB Color Space Distribution\", fontweight='bold')\naxes[0,1].set_ylabel(\"Mean Channel Value\")\naxes[0,1].grid(alpha=0.3)\n\n# Plant-specific Saturation (indicator of leaf health/vibrancy)\nplant_names = list(color_by_plant_advanced.keys())\nsaturation_vals = [color_by_plant_advanced[p]['S'] for p in plant_names]\n\naxes[1,0].barh(plant_names, saturation_vals, color='green', alpha=0.7)\naxes[1,0].set_title(\"Average Saturation by Plant Species\", fontweight='bold')\naxes[1,0].set_xlabel(\"Saturation Value\")\naxes[1,0].grid(alpha=0.3)\n\n# Plant-specific A channel (green-red balance)\na_channel_vals = [color_by_plant_advanced[p]['A'] for p in plant_names]\n\naxes[1,1].barh(plant_names, a_channel_vals, color='red', alpha=0.7)\naxes[1,1].axvline(x=128, color='black', linestyle='--', label='Neutral (128)')\naxes[1,1].set_title(\"A* Channel (Green-Red Balance) by Plant\", fontweight='bold')\naxes[1,1].set_xlabel(\"A* Value (Lower = Greener)\")\naxes[1,1].legend()\naxes[1,1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\033[1m\\nADVANCED COLOR INSIGHTS:\\033[0m\")\nprint(f\"Saturation Range: {np.min(S_vals):.1f}-{np.max(S_vals):.1f} (indicates leaf vibrancy)\")\nprint(f\"A* Channel Mean: {np.mean(A_vals):.1f} (lower values = more green dominance)\")\nprint(f\"Color spaces reveal subtle species differences not visible in RGB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:43.542914Z","iopub.execute_input":"2025-10-25T20:10:43.543225Z","iopub.status.idle":"2025-10-25T20:10:45.786718Z","shell.execute_reply.started":"2025-10-25T20:10:43.543203Z","shell.execute_reply":"2025-10-25T20:10:45.785683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2>Augmentation Analysis for Medicinal Plants</h2>**","metadata":{}},{"cell_type":"code","source":"print(\"\\033[1m\\nAUGMENTATION PROBE FOR MEDICINAL PLANT IDENTIFICATION:\\n\\033[0m\")\n# Test on multiple plant types to see generalization\ntest_plants = ['Kalochitra', 'Punarnava', 'Shotomuli']  # Diverse examples\n\nfig, axes = plt.subplots(len(test_plants), 7, figsize=(20, 3*len(test_plants)))\n\nif len(test_plants) == 1:\n    axes = [axes]\n\nfor row, plant_name in enumerate(test_plants):\n    # Find the type for this plant\n    type_name = [k for k, v in plant_mapping.items() if v == plant_name][0]\n    class_path = os.path.join(dataset_path, type_name)\n    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    \n    if images:\n        sample_path = os.path.join(class_path, images[0])\n        original_img = Image.open(sample_path)\n        \n        # Define augmentations\n        augmentations = {\n            'Original': original_img,\n            'Horizontal Flip': original_img.transpose(Image.FLIP_LEFT_RIGHT),\n            'Vertical Flip': original_img.transpose(Image.FLIP_TOP_BOTTOM),\n            'Rotation (15°)': original_img.rotate(15, expand=True),\n            'Rotation (45°)': original_img.rotate(45, expand=True),\n            'Brightness +30%': Image.eval(original_img, lambda x: min(255, x * 1.3)),\n            'Color Jitter': Image.eval(original_img, lambda x: min(255, (x - 128) * 1.2 + 128)),\n        }\n        \n        for col, (aug_name, aug_img) in enumerate(augmentations.items()):\n            axes[row][col].imshow(aug_img)\n            if row == 0:  # Title only on first row\n                axes[row][col].set_title(aug_name, fontweight='bold', fontsize=10)\n            axes[row][col].axis('off')\n            if col == 0:  # Plant name on first column\n                axes[row][col].text(-0.3, 0.5, plant_name, transform=axes[row][col].transAxes,\n                                  fontsize=12, fontweight='bold', va='center', ha='right')\n\nplt.suptitle('Augmentation Effects on Different Medicinal Plants\\n\\n', fontsize=16, fontweight='bold', y=0.95)\nplt.tight_layout()\nplt.show()\n\n# Safety Analysis\nprint(\"\"\"\n\\033[1mSAFE AUGMENTATIONS:\\033[0m\n\n  Horizontal Flip \n  - Preserves leaf symmetry and morphology\n  - Maintains medicinal identification features\n  \n  Small Rotations (±15-20°)\n  - Represents natural leaf orientation variations\n  - Doesn't distort key morphological features\n  \n  Brightness/Contrast Adjustments (±20-30%)\n  - Simulates different lighting conditions\n  - Maintains color integrity for identification\n\n  Mild Color Preservation\n  - Small saturation adjustments\n  - Maintains species-specific color patterns\n\n\\033[1mCAUTION REQUIRED AUGMENTATIONS:\\033[0m\n\n  Large Rotations (>30°)\n  - May change leaf orientation semantics\n  - Could affect vein pattern recognition\n  \n  Vertical Flip\n  - Unnatural for leaf images\n  - May confuse some CNN architectures\n\n  Extreme Cropping\n  - Might remove important leaf parts\n  - Could eliminate diagnostic features\n\n\\033[1mPOTENTIALLY HARMFUL AUGMENTATIONS:\\033[0m\n\n  Extreme Color Jitter\n  - Could alter important color-based identification\n  - May change leaf appearance significantly\n  \n  Shearing/Warping\n  - Distorts natural leaf morphology\n  - Affects shape-based classification\n  \n  Extreme Brightness (>50% change)\n  - Washes out important texture details\n  - Reduces feature discriminability\n\"\"\")\n\nprint(\"\\033[1m\\nFINAL AUGMENTATION STRATEGY:\\033[0m\")\nprint(\"\")\nprint(\"• Use: Horizontal flips, small rotations, mild brightness/contrast\")\nprint(\"• Avoid: Vertical flips, extreme color changes, heavy cropping\")\nprint(\"• Test: Monitor validation performance with different augmentations\")\nprint(\"• Balance: Combine safe augmentations for robust model training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:45.787749Z","iopub.execute_input":"2025-10-25T20:10:45.788054Z","iopub.status.idle":"2025-10-25T20:10:47.875567Z","shell.execute_reply.started":"2025-10-25T20:10:45.788026Z","shell.execute_reply":"2025-10-25T20:10:47.874423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**<h2>ENHANCED AUGMENTATION & ROBUSTNESS ANALYSIS</h2>**","metadata":{}},{"cell_type":"code","source":"def add_gaussian_noise(image, mean=0, sigma=25):\n    \"\"\"Add Gaussian noise to test model robustness\"\"\"\n    noise = np.random.normal(mean, sigma, image.shape).astype(np.int16)\n    noisy_img = np.clip(image.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n    return noisy_img\n\ndef analyze_background_dominance(img_path):\n    \"\"\"Analyze how much of the image is background vs plant\"\"\"\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Calculate background percentage (assuming light backgrounds)\n    background_pixels = np.sum(gray > 200)  # threshold for light background\n    percent_bg = (background_pixels / gray.size) * 100\n    \n    return percent_bg\n\n# Test on representative samples\ntest_samples = [\n    ('type_1', 'Kalochitra'),\n    ('type_6', 'Kalamegh'), \n    ('type_11', 'Punarnava'),\n    ('type_15', 'Shotomuli')  # Include a minority class\n]\n\n# Background analysis\nprint(\"\\033[1mBACKGROUND ANALYSIS:\\033[0m\")\nprint(\"\")\nbg_percentages = []\n\nfor type_name, plant_name in test_samples:\n    class_path = os.path.join(dataset_path, type_name)\n    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    if images:\n        img_path = os.path.join(class_path, images[0])\n        bg_percent = analyze_background_dominance(img_path)\n        bg_percentages.append(bg_percent)\n        status = \"LOW\" if bg_percent < 30 else \"HIGH\"\n        print(f\"  {plant_name}: {bg_percent:.1f}% background ({status})\")\n\nplt.figure(figsize=(10, 5))\nplt.hist(bg_percentages, bins=15, color='lightblue', edgecolor='black', alpha=0.7)\nplt.axvline(x=30, color='red', linestyle='--', label='Ideal Threshold (30%)')\nplt.title(\"Background Dominance Distribution\", fontweight='bold')\nplt.xlabel(\"Background Percentage (%)\")\nplt.ylabel(\"Number of Samples\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Comprehensive augmentation robustness test\nprint(\"\\033[1m\\nROBUSTNESS TESTING WITH ADVANCED AUGMENTATIONS:\\033[0m\")\nprint(\"\")\n\nfor type_name, plant_name in test_samples[:2]:  # Test on 2 plants\n    class_path = os.path.join(dataset_path, type_name)\n    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    \n    if images:\n        img_path = os.path.join(class_path, images[0])\n        img = cv2.imread(img_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Create various augmentations\n        from torchvision import transforms as T\n        \n        transform_flip = T.RandomHorizontalFlip(p=1)\n        transform_rot = T.RandomRotation(20)\n        transform_color = T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2)\n        transform_pil = T.ToPILImage()\n        \n        # Convert to PIL for torchvision transforms\n        img_pil = transform_pil(img_rgb)\n        \n        # Apply augmentations\n        flipped = transform_flip(img_pil)\n        rotated = transform_rot(img_pil)\n        color_jittered = transform_color(img_pil)\n        \n        # Advanced augmentations\n        noisy_img = add_gaussian_noise(img_rgb)\n        \n        # Zoom augmentation (center crop + resize)\n        h, w = img_rgb.shape[:2]\n        crop_ratio = 0.15\n        crop = img_rgb[int(h*crop_ratio):int(h*(1-crop_ratio)), \n                       int(w*crop_ratio):int(w*(1-crop_ratio))]\n        zoomed = cv2.resize(crop, (w, h))\n        \n        # Display all augmentations\n        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n        \n        augmentations = [\n            (\"Original\", img_rgb),\n            (\"Horizontal Flip\", np.array(flipped)),\n            (\"Rotation 20°\", np.array(rotated)),\n            (\"Color Jitter\", np.array(color_jittered)),\n            (\"Gaussian Noise\", noisy_img),\n            (\"Zoom (15% crop)\", zoomed),\n            (\"Brightness +40%\", np.array(T.ColorJitter(brightness=0.4)(img_pil))),\n            (\"Contrast +40%\", np.array(T.ColorJitter(contrast=0.4)(img_pil)))\n        ]\n        \n        for i, (title, aug_img) in enumerate(augmentations):\n            row, col = i // 4, i % 4\n            axes[row, col].imshow(aug_img)\n            axes[row, col].set_title(title, fontsize=10, fontweight='bold')\n            axes[row, col].axis('off')\n        \n        plt.suptitle(f\"Advanced Augmentation Robustness: {plant_name}\", fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\nprint(\"\\033[1m\\nENHANCED AUGMENTATION STRATEGY:\\033[0m\")\nprint(\"\")\nprint(\"SAFE & RECOMMENDED:\")\nprint(\"•Horizontal Flips\")\nprint(\"• Small Rotations (15-25°)\")\nprint(\"• Mild Color Jitter (brightness/contrast ±30%, saturation ±20%)\")\nprint(\"• Gaussian Noise (σ=15-25) - improves robustness\")\nprint(\"• Center Zoom (10-15% crop) - focuses on leaf details\")\nprint(\"\")\nprint(\"USE JUDICIOUSLY:\")\nprint(\"• Larger Rotations (>30°) - monitor performance\")\nprint(\"• Heavy Color Changes - may affect species identification\")\nprint(\"• Extreme Cropping (>20%) - might remove key features\")\nprint(\"\")\nprint(\"INSIGHTS FROM ADVANCED ANALYSIS:\")\nprint(\"• Backgrounds are generally clean (<30% background)\")\nprint(\"• HSV/LAB spaces reveal subtle species differences\")\nprint(\"• Noise robustness can be built with Gaussian augmentation\")\nprint(\"• Most plants maintain identifiable features under reasonable transformations\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:10:47.876818Z","iopub.execute_input":"2025-10-25T20:10:47.877091Z","iopub.status.idle":"2025-10-25T20:10:50.734897Z","shell.execute_reply.started":"2025-10-25T20:10:47.877070Z","shell.execute_reply":"2025-10-25T20:10:50.734003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}